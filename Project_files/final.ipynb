{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RIDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAKE CURRENCY DETECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pywt\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wavelet Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_wavelet_features(image_path):\n",
    "  # Load the currency image\n",
    "  image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "  # Apply DWT using Daubechies 1 (DB1) wavelet\n",
    "  coeffs = pywt.dwt2(image, 'db1')\n",
    "  approx_coefficients = coeffs[0]\n",
    "\n",
    "  # Calculate variance, skewness, and kurtosis\n",
    "  variance = np.var(approx_coefficients)\n",
    "  skewness = np.mean((approx_coefficients - np.mean(approx_coefficients))**3) / np.std(approx_coefficients)**3\n",
    "  kurtosis = np.mean((approx_coefficients - np.mean(approx_coefficients))**4) / np.std(approx_coefficients)**4\n",
    "\n",
    "  # Calculate entropy\n",
    "  entropy = scipy.stats.entropy(approx_coefficients.flatten())\n",
    "\n",
    "  # Create an array with the extracted features\n",
    "  feature_array = np.array([variance, skewness, kurtosis, entropy])\n",
    "\n",
    "  return feature_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 500 Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'D:\\\\RIDS DATASET\\\\500 Real'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 29\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(all_features)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m features_matrix_500_real \u001b[38;5;241m=\u001b[39m process_folder(folder_path_500_real)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of feature matrix:\u001b[39m\u001b[38;5;124m\"\u001b[39m, features_matrix_500_real\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[1;32mIn[6], line 19\u001b[0m, in \u001b[0;36mprocess_folder\u001b[1;34m(folder_path_500_real)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Processes all images in a folder and extracts wavelet features.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m                containing extracted features for all images.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m all_features \u001b[38;5;241m=\u001b[39m []  \u001b[38;5;66;03m# Array to  store features of all images\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(folder_path_500_real):\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# Check for image file\u001b[39;00m\n\u001b[0;32m     21\u001b[0m         image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_path_500_real, filename)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'D:\\\\RIDS DATASET\\\\500 Real'"
     ]
    }
   ],
   "source": [
    "#for 500 real\n",
    "\n",
    "folder_path_500_real =r'D:\\RIDS DATASET\\500 Real' \n",
    "\n",
    "\n",
    "def process_folder(folder_path_500_real):\n",
    "    \"\"\"Processes all images in a folder and extracts wavelet features.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing images.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array with shape (number_of_images, number_of_features) \n",
    "                    containing extracted features for all images.\n",
    "    \"\"\"\n",
    "\n",
    "    all_features = []  # Array to  store features of all images\n",
    "\n",
    "    for filename in os.listdir(folder_path_500_real):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):  # Check for image file\n",
    "            image_path = os.path.join(folder_path_500_real, filename)\n",
    "            features = extract_wavelet_features(image_path)\n",
    "            all_features.append(features)\n",
    "\n",
    "    return np.array(all_features)\n",
    "\n",
    "# Example usage\n",
    "\n",
    "features_matrix_500_real = process_folder(folder_path_500_real)\n",
    "print(\"Shape of feature matrix:\", features_matrix_500_real.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 500 Fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'entropy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 27\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(all_features)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m features_matrix_500_fake \u001b[38;5;241m=\u001b[39m process_folder(folder_path_500_fake)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of feature matrix:\u001b[39m\u001b[38;5;124m\"\u001b[39m, features_matrix_500_fake\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[1;32mIn[9], line 20\u001b[0m, in \u001b[0;36mprocess_folder\u001b[1;34m(folder_path_500_fake)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# Check for image file\u001b[39;00m\n\u001b[0;32m     19\u001b[0m         image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_path_500_fake, filename)\n\u001b[1;32m---> 20\u001b[0m         features \u001b[38;5;241m=\u001b[39m extract_wavelet_features(image_path)\n\u001b[0;32m     21\u001b[0m         all_features\u001b[38;5;241m.\u001b[39mappend(features)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(all_features)\n",
      "Cell \u001b[1;32mIn[6], line 18\u001b[0m, in \u001b[0;36mextract_wavelet_features\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m     12\u001b[0m kurtosis \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean((approx_coefficients \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(approx_coefficients))\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m4\u001b[39m) \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mstd(approx_coefficients)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m4\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Calculate entropy\u001b[39;00m\n\u001b[0;32m     15\u001b[0m  \n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Create an array with the extracted features\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m feature_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([variance, skewness, kurtosis, entropy])\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m feature_array\n",
      "\u001b[1;31mNameError\u001b[0m: name 'entropy' is not defined"
     ]
    }
   ],
   "source": [
    "folder_path_500_fake =r'D:\\RIDS DATASET\\500 FAKE' \n",
    "\n",
    "\n",
    "def process_folder(folder_path_500_fake):\n",
    "    \"\"\"Processes all images in a folder and extracts wavelet features.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing images.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array with shape (number_of_images, number_of_features) \n",
    "                    containing extracted features for all images.\n",
    "    \"\"\"\n",
    "\n",
    "    all_features = []  # Array to  store features of all images\n",
    "\n",
    "    for filename in os.listdir(folder_path_500_fake):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):  # Check for image file\n",
    "            image_path = os.path.join(folder_path_500_fake, filename)\n",
    "            features = extract_wavelet_features(image_path)\n",
    "            all_features.append(features)\n",
    "\n",
    "    return np.array(all_features)\n",
    "\n",
    "# Example usage\n",
    "\n",
    "features_matrix_500_fake = process_folder(folder_path_500_fake)\n",
    "print(\"Shape of feature matrix:\", features_matrix_500_fake.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2000 Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature matrix: (1715, 4)\n"
     ]
    }
   ],
   "source": [
    "folder_path_2000_real =r'D:\\RIDS DATASET\\2000 Real' \n",
    "\n",
    "\n",
    "def process_folder(folder_path_2000_real):\n",
    "    \"\"\"Processes all images in a folder and extracts wavelet features.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing images.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array with shape (number_of_images, number_of_features) \n",
    "                    containing extracted features for all images.\n",
    "    \"\"\"\n",
    "\n",
    "    all_features = []  # Array to  store features of all images\n",
    "\n",
    "    for filename in os.listdir(folder_path_2000_real):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):  # Check for image file\n",
    "            image_path = os.path.join(folder_path_2000_real, filename)\n",
    "            features = extract_wavelet_features(image_path)\n",
    "            all_features.append(features)\n",
    "\n",
    "    return np.array(all_features)\n",
    "\n",
    "# Example usage\n",
    "\n",
    "features_matrix_2000_real = process_folder(folder_path_2000_real)\n",
    "print(\"Shape of feature matrix:\", features_matrix_2000_real.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2000 Fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path_2000_real =r'D:\\RIDS DATASET\\2000 FAKE' \n",
    "\n",
    "\n",
    "def process_folder(folder_path_2000_fake):\n",
    "    \"\"\"Processes all images in a folder and extracts wavelet features.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing images.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array with shape (number_of_images, number_of_features) \n",
    "                    containing extracted features for all images.\n",
    "    \"\"\"\n",
    "\n",
    "    all_features = []  # Array to  store features of all images\n",
    "\n",
    "    for filename in os.listdir(folder_path_2000_fake):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):  # Check for image file\n",
    "            image_path = os.path.join(folder_path_2000_fake, filename)\n",
    "            features = extract_wavelet_features(image_path)\n",
    "            all_features.append(features)\n",
    "\n",
    "    return np.array(all_features)\n",
    "\n",
    "# Example usage\n",
    "\n",
    "features_matrix_2000_fake = process_folder(folder_path_2000_fake)\n",
    "print(\"Shape of feature matrix:\", features_matrix_2000_fake.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 500 Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance: [10210.68096852  4599.77989154  3924.41462396 ... 12077.66484254\n",
      " 12077.66484254 12077.66484254]\n",
      "Skewness: [-1.13483951 -0.22621542 -0.40036064 ... -0.59252536 -0.59252536\n",
      " -0.59252536]\n",
      "Kurtosis: [3.88440606 2.94017692 2.88401227 ... 2.70125656 2.70125656 2.70125656]\n",
      "Entropy: [-5.24372919e+08 -2.36374569e+07 -2.62826170e+07 ... -7.56298467e+07\n",
      " -7.56298467e+07 -7.56298467e+07]\n",
      "          Variance  Skewness  Kurtosis       Entropy\n",
      "0     10210.680969 -1.134840  3.884406 -5.243729e+08\n",
      "1      4599.779892 -0.226215  2.940177 -2.363746e+07\n",
      "2      3924.414624 -0.400361  2.884012 -2.628262e+07\n",
      "3     10210.680969 -1.134840  3.884406 -5.243729e+08\n",
      "4     10210.680969 -1.134840  3.884406 -5.243729e+08\n",
      "...            ...       ...       ...           ...\n",
      "1671  12077.664843 -0.592525  2.701257 -7.562985e+07\n",
      "1672  12164.772167 -0.743464  2.843340 -6.609715e+07\n",
      "1673  12077.664843 -0.592525  2.701257 -7.562985e+07\n",
      "1674  12077.664843 -0.592525  2.701257 -7.562985e+07\n",
      "1675  12077.664843 -0.592525  2.701257 -7.562985e+07\n",
      "\n",
      "[1676 rows x 4 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variance</th>\n",
       "      <th>Skewness</th>\n",
       "      <th>Kurtosis</th>\n",
       "      <th>Entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10210.680969</td>\n",
       "      <td>-1.134840</td>\n",
       "      <td>3.884406</td>\n",
       "      <td>-5.243729e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4599.779892</td>\n",
       "      <td>-0.226215</td>\n",
       "      <td>2.940177</td>\n",
       "      <td>-2.363746e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3924.414624</td>\n",
       "      <td>-0.400361</td>\n",
       "      <td>2.884012</td>\n",
       "      <td>-2.628262e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10210.680969</td>\n",
       "      <td>-1.134840</td>\n",
       "      <td>3.884406</td>\n",
       "      <td>-5.243729e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10210.680969</td>\n",
       "      <td>-1.134840</td>\n",
       "      <td>3.884406</td>\n",
       "      <td>-5.243729e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1671</th>\n",
       "      <td>12077.664843</td>\n",
       "      <td>-0.592525</td>\n",
       "      <td>2.701257</td>\n",
       "      <td>-7.562985e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1672</th>\n",
       "      <td>12164.772167</td>\n",
       "      <td>-0.743464</td>\n",
       "      <td>2.843340</td>\n",
       "      <td>-6.609715e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1673</th>\n",
       "      <td>12077.664843</td>\n",
       "      <td>-0.592525</td>\n",
       "      <td>2.701257</td>\n",
       "      <td>-7.562985e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674</th>\n",
       "      <td>12077.664843</td>\n",
       "      <td>-0.592525</td>\n",
       "      <td>2.701257</td>\n",
       "      <td>-7.562985e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1675</th>\n",
       "      <td>12077.664843</td>\n",
       "      <td>-0.592525</td>\n",
       "      <td>2.701257</td>\n",
       "      <td>-7.562985e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1676 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Variance  Skewness  Kurtosis       Entropy\n",
       "0     10210.680969 -1.134840  3.884406 -5.243729e+08\n",
       "1      4599.779892 -0.226215  2.940177 -2.363746e+07\n",
       "2      3924.414624 -0.400361  2.884012 -2.628262e+07\n",
       "3     10210.680969 -1.134840  3.884406 -5.243729e+08\n",
       "4     10210.680969 -1.134840  3.884406 -5.243729e+08\n",
       "...            ...       ...       ...           ...\n",
       "1671  12077.664843 -0.592525  2.701257 -7.562985e+07\n",
       "1672  12164.772167 -0.743464  2.843340 -6.609715e+07\n",
       "1673  12077.664843 -0.592525  2.701257 -7.562985e+07\n",
       "1674  12077.664843 -0.592525  2.701257 -7.562985e+07\n",
       "1675  12077.664843 -0.592525  2.701257 -7.562985e+07\n",
       "\n",
       "[1676 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pywt\n",
    "\n",
    "# Define the path to the image dataset folder\n",
    "dataset_folder = r'D:\\RIDS DATASET\\500 Real' \n",
    "\n",
    "# Initialize empty arrays to store features\n",
    "variance_array = []\n",
    "skewness_array = []\n",
    "kurtosis_array = []\n",
    "entropy_array = []\n",
    "\n",
    "# Iterate over each image in the dataset folder\n",
    "for filename in os.listdir(dataset_folder):\n",
    "    if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "        image_path = os.path.join(dataset_folder, filename)\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Read image in grayscale\n",
    "\n",
    "        # Perform wavelet transform (using Haar wavelet)\n",
    "        coeffs = pywt.dwt2(image, 'db1')\n",
    "        LL, (LH, HL, HH) = coeffs\n",
    "\n",
    "        # Calculate variance, skewness, kurtosis, and entropy\n",
    "        variance = np.var(LL)\n",
    "        skewness = np.mean((LL - np.mean(LL))**3) / np.std(LL)**3\n",
    "        kurtosis = np.mean((LL - np.mean(LL))**4) / np.std(LL)**4\n",
    "        entropy = -np.sum(LL * np.log2(LL + 1e-10))\n",
    "\n",
    "        # Append features to arrays\n",
    "        variance_array.append(variance)\n",
    "        skewness_array.append(skewness)\n",
    "        kurtosis_array.append(kurtosis)\n",
    "        entropy_array.append(entropy)\n",
    "\n",
    "# Convert arrays to numpy arrays\n",
    "variance_array = np.array(variance_array)\n",
    "skewness_array = np.array(skewness_array)\n",
    "kurtosis_array = np.array(kurtosis_array)\n",
    "entropy_array = np.array(entropy_array)\n",
    "\n",
    "# Print the extracted features\n",
    "print(f\"Variance: {variance_array}\")\n",
    "print(f\"Skewness: {skewness_array}\")\n",
    "print(f\"Kurtosis: {kurtosis_array}\")\n",
    "print(f\"Entropy: {entropy_array}\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame to store the features\n",
    "df = pd.DataFrame({\n",
    "    \"Variance\": variance_array,\n",
    "    \"Skewness\": skewness_array,\n",
    "    \"Kurtosis\": kurtosis_array,\n",
    "    \"Entropy\": entropy_array\n",
    "})\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2000 Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance: [4992.56395164 4992.56395164 4992.56395164 ... 8396.70564462 8396.70564462\n",
      " 8396.70564462]\n",
      "Skewness: [-0.47800152 -0.47800152 -0.47800152 ... -0.95730402 -0.95730402\n",
      " -0.95730402]\n",
      "Kurtosis: [3.17837714 3.17837714 3.17837714 ... 3.72771863 3.72771863 3.72771863]\n",
      "Entropy: [-4.86387135e+07 -4.86387135e+07 -4.86387135e+07 ... -5.82877514e+08\n",
      " -5.82877514e+08 -5.82877514e+08]\n",
      "          Variance  Skewness  Kurtosis       Entropy\n",
      "0     10210.680969 -1.134840  3.884406 -5.243729e+08\n",
      "1      4599.779892 -0.226215  2.940177 -2.363746e+07\n",
      "2      3924.414624 -0.400361  2.884012 -2.628262e+07\n",
      "3     10210.680969 -1.134840  3.884406 -5.243729e+08\n",
      "4     10210.680969 -1.134840  3.884406 -5.243729e+08\n",
      "...            ...       ...       ...           ...\n",
      "1671  12077.664843 -0.592525  2.701257 -7.562985e+07\n",
      "1672  12164.772167 -0.743464  2.843340 -6.609715e+07\n",
      "1673  12077.664843 -0.592525  2.701257 -7.562985e+07\n",
      "1674  12077.664843 -0.592525  2.701257 -7.562985e+07\n",
      "1675  12077.664843 -0.592525  2.701257 -7.562985e+07\n",
      "\n",
      "[1676 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pywt\n",
    "\n",
    "# Define the path to the image dataset folder\n",
    "dataset_folder = r'D:\\RIDS DATASET\\2000 Real' \n",
    "\n",
    "# Initialize empty arrays to store features\n",
    "variance_array = []\n",
    "skewness_array = []\n",
    "kurtosis_array = []\n",
    "entropy_array = []\n",
    "\n",
    "# Iterate over each image in the dataset folder\n",
    "for filename in os.listdir(dataset_folder):\n",
    "    if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "        image_path = os.path.join(dataset_folder, filename)\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Read image in grayscale\n",
    "\n",
    "        # Perform wavelet transform (using Haar wavelet)\n",
    "        coeffs = pywt.dwt2(image, 'db1')\n",
    "        LL, (LH, HL, HH) = coeffs\n",
    "\n",
    "        # Calculate variance, skewness, kurtosis, and entropy\n",
    "        variance = np.var(LL)\n",
    "        skewness = np.mean((LL - np.mean(LL))**3) / np.std(LL)**3\n",
    "        kurtosis = np.mean((LL - np.mean(LL))**4) / np.std(LL)**4\n",
    "        entropy = -np.sum(LL * np.log2(LL + 1e-10))\n",
    "\n",
    "        # Append features to arrays\n",
    "        variance_array.append(variance)\n",
    "        skewness_array.append(skewness)\n",
    "        kurtosis_array.append(kurtosis)\n",
    "        entropy_array.append(entropy)\n",
    "\n",
    "# Convert arrays to numpy arrays\n",
    "variance_array = np.array(variance_array)\n",
    "skewness_array = np.array(skewness_array)\n",
    "kurtosis_array = np.array(kurtosis_array)\n",
    "entropy_array = np.array(entropy_array)\n",
    "\n",
    "# Print the extracted features\n",
    "print(f\"Variance: {variance_array}\")\n",
    "print(f\"Skewness: {skewness_array}\")\n",
    "print(f\"Kurtosis: {kurtosis_array}\")\n",
    "print(f\"Entropy: {entropy_array}\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame to store the features\n",
    "df_2000 = pd.DataFrame({\n",
    "    \"Variance\": variance_array,\n",
    "    \"Skewness\": skewness_array,\n",
    "    \"Kurtosis\": kurtosis_array,\n",
    "    \"Entropy\": entropy_array\n",
    "})\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2000 Fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance: [ 7015.89972026  7979.5927965   7979.5927965   7979.5927965\n",
      "  7979.5927965   6982.94502536  6982.94502536  6982.94502536\n",
      "  6982.94502536  5401.17774084  5401.17774084  5401.17774084\n",
      "  5401.17774084  8038.3725821   8038.3725821   8038.3725821\n",
      "  8038.3725821   6917.6257618   6917.6257618   6917.6257618\n",
      "  6917.6257618   7156.55604634  7156.55604634  7156.55604634\n",
      "  7156.55604634  7193.28567732  7193.28567732  7193.28567732\n",
      "  7193.28567732  5889.00594213  5889.00594213  5889.00594213\n",
      "  5889.00594213  6013.32410303  6013.32410303  6013.32410303\n",
      "  6013.32410303  5841.43998151  5841.43998151  5841.43998151\n",
      "  5841.43998151  7015.89972026  7530.49386344  7530.49386344\n",
      "  7530.49386344  7530.49386344  7015.89972026 11984.26535908\n",
      "  6982.94502536  5401.17774084  8038.3725821   6917.6257618\n",
      "  7156.55604634  7193.28567732 14717.51313903  7979.5927965\n",
      "  7015.89972026  7979.5927965  16471.56626195 26417.78491072\n",
      "  7233.00612142 25263.35651532 19632.54718179  9267.78066639\n",
      "  3666.24354906  6896.26440698  8092.24130716 24473.93836424\n",
      "  7233.00612142  7979.5927965  16471.56626195 24473.93836424\n",
      " 19632.54718179  7015.89972026  7979.5927965   7233.00612142\n",
      "  7539.14764531  7539.14764531  8157.9652854  11821.47356132\n",
      " 12850.42481166  8277.82549124  7156.55604634  6013.32410303\n",
      "  5841.43998151  5841.43998151  5841.43998151  7193.28567732\n",
      "  6982.94502536  5889.00594213  5401.17774084  8038.3725821\n",
      "  6013.32410303]\n",
      "Skewness: [-0.75847749 -1.22215236 -1.22215236 -1.22215236 -1.22215236 -0.55958447\n",
      " -0.55958447 -0.55958447 -0.55958447 -0.86910772 -0.86910772 -0.86910772\n",
      " -0.86910772 -1.03468896 -1.03468896 -1.03468896 -1.03468896 -0.57296698\n",
      " -0.57296698 -0.57296698 -0.57296698 -0.97804244 -0.97804244 -0.97804244\n",
      " -0.97804244 -1.13289365 -1.13289365 -1.13289365 -1.13289365 -1.40291825\n",
      " -1.40291825 -1.40291825 -1.40291825 -1.53921093 -1.53921093 -1.53921093\n",
      " -1.53921093 -1.48897874 -1.48897874 -1.48897874 -1.48897874 -0.75847749\n",
      " -1.0774893  -1.0774893  -1.0774893  -1.0774893  -0.75847749 -1.24440919\n",
      " -0.55958447 -0.86910772 -1.03468896 -0.57296698 -0.97804244 -1.13289365\n",
      " -0.03443048 -1.22215236 -0.75847749 -1.22215236 -0.66493498 -0.02204402\n",
      " -0.29388261 -0.86609729 -0.56356781 -1.0503222  -1.80630988 -0.05829463\n",
      "  0.01118064 -0.64772836 -0.29388261 -1.22215236 -0.66493498 -0.64772836\n",
      " -0.56356781 -0.75847749 -1.22215236 -0.29388261 -0.61276934 -0.61276934\n",
      " -0.36215239  0.1934956  -0.97981742 -0.32932883 -0.97804244 -1.53921093\n",
      " -1.48897874 -1.48897874 -1.48897874 -1.13289365 -0.55958447 -1.40291825\n",
      " -0.86910772 -1.03468896 -1.53921093]\n",
      "Kurtosis: [2.5452236  4.87910865 4.87910865 4.87910865 4.87910865 3.18077109\n",
      " 3.18077109 3.18077109 3.18077109 3.1911898  3.1911898  3.1911898\n",
      " 3.1911898  3.98297841 3.98297841 3.98297841 3.98297841 3.22680295\n",
      " 3.22680295 3.22680295 3.22680295 3.34786127 3.34786127 3.34786127\n",
      " 3.34786127 3.77721496 3.77721496 3.77721496 3.77721496 5.75421213\n",
      " 5.75421213 5.75421213 5.75421213 6.02723785 6.02723785 6.02723785\n",
      " 6.02723785 5.80952175 5.80952175 5.80952175 5.80952175 2.5452236\n",
      " 4.00074842 4.00074842 4.00074842 4.00074842 2.5452236  3.47376848\n",
      " 3.18077109 3.1911898  3.98297841 3.22680295 3.34786127 3.77721496\n",
      " 1.7583329  4.87910865 2.5452236  4.87910865 2.71456378 1.53423114\n",
      " 2.03701153 2.71417578 1.82606828 3.18186008 7.12258963 1.97959735\n",
      " 2.30843195 2.03927914 2.03701153 4.87910865 2.71456378 2.03927914\n",
      " 1.82606828 2.5452236  4.87910865 2.03701153 2.43967265 2.43967265\n",
      " 3.19002114 2.94930149 3.83887837 2.22555927 3.34786127 6.02723785\n",
      " 5.80952175 5.80952175 5.80952175 3.77721496 3.18077109 5.75421213\n",
      " 3.1911898  3.98297841 6.02723785]\n",
      "Entropy: [-1.77177118e+08 -1.82066943e+08 -1.82066943e+08 -1.82066943e+08\n",
      " -1.82066943e+08 -1.18297224e+09 -1.18297224e+09 -1.18297224e+09\n",
      " -1.18297224e+09 -4.89294745e+08 -4.89294745e+08 -4.89294745e+08\n",
      " -4.89294745e+08 -5.75434176e+08 -5.75434176e+08 -5.75434176e+08\n",
      " -5.75434176e+08 -1.18465979e+09 -1.18465979e+09 -1.18465979e+09\n",
      " -1.18465979e+09 -1.15292889e+09 -1.15292889e+09 -1.15292889e+09\n",
      " -1.15292889e+09 -2.08341825e+09 -2.08341825e+09 -2.08341825e+09\n",
      " -2.08341825e+09 -2.41097714e+09 -2.41097714e+09 -2.41097714e+09\n",
      " -2.41097714e+09 -2.35243480e+09 -2.35243480e+09 -2.35243480e+09\n",
      " -2.35243480e+09 -2.35950124e+09 -2.35950124e+09 -2.35950124e+09\n",
      " -2.35950124e+09 -1.77177118e+08 -1.10699709e+08 -1.10699709e+08\n",
      " -1.10699709e+08 -1.10699709e+08 -1.77177118e+08 -2.37542001e+08\n",
      " -1.18297224e+09 -4.89294745e+08 -5.75434176e+08 -1.18465979e+09\n",
      " -1.15292889e+09 -2.08341825e+09 -2.46751056e+07 -1.82066943e+08\n",
      " -1.77177118e+08 -1.82066943e+08 -1.07114255e+08 -4.13400420e+07\n",
      " -2.00752168e+08 -1.77780692e+08 -9.17547027e+07 -7.96764555e+07\n",
      " -4.45906596e+07 -3.44377796e+07 -1.04512238e+08 -5.07888443e+08\n",
      " -2.00752168e+08 -1.82066943e+08 -1.07114255e+08 -5.07888443e+08\n",
      " -9.17547027e+07 -1.77177118e+08 -1.82066943e+08 -2.00752168e+08\n",
      " -4.63481162e+08 -4.63481162e+08 -9.22202746e+07 -1.00907916e+08\n",
      " -9.86781834e+07 -2.88451907e+08 -1.15292889e+09 -2.35243480e+09\n",
      " -2.35950124e+09 -2.35950124e+09 -2.35950124e+09 -2.08341825e+09\n",
      " -1.18297224e+09 -2.41097714e+09 -4.89294745e+08 -5.75434176e+08\n",
      " -2.35243480e+09]\n",
      "          Variance  Skewness  Kurtosis       Entropy\n",
      "0     10210.680969 -1.134840  3.884406 -5.243729e+08\n",
      "1      4599.779892 -0.226215  2.940177 -2.363746e+07\n",
      "2      3924.414624 -0.400361  2.884012 -2.628262e+07\n",
      "3     10210.680969 -1.134840  3.884406 -5.243729e+08\n",
      "4     10210.680969 -1.134840  3.884406 -5.243729e+08\n",
      "...            ...       ...       ...           ...\n",
      "1671  12077.664843 -0.592525  2.701257 -7.562985e+07\n",
      "1672  12164.772167 -0.743464  2.843340 -6.609715e+07\n",
      "1673  12077.664843 -0.592525  2.701257 -7.562985e+07\n",
      "1674  12077.664843 -0.592525  2.701257 -7.562985e+07\n",
      "1675  12077.664843 -0.592525  2.701257 -7.562985e+07\n",
      "\n",
      "[1676 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pywt\n",
    "\n",
    "# Define the path to the image dataset folder\n",
    "dataset_folder = r'D:\\RIDS DATASET\\2000 FAKE' \n",
    "\n",
    "# Initialize empty arrays to store features\n",
    "variance_array = []\n",
    "skewness_array = []\n",
    "kurtosis_array = []\n",
    "entropy_array = []\n",
    "\n",
    "# Iterate over each image in the dataset folder\n",
    "for filename in os.listdir(dataset_folder):\n",
    "    if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "        image_path = os.path.join(dataset_folder, filename)\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Read image in grayscale\n",
    "\n",
    "        # Perform wavelet transform (using Haar wavelet)\n",
    "        coeffs = pywt.dwt2(image, 'db1')\n",
    "        LL, (LH, HL, HH) = coeffs\n",
    "\n",
    "        # Calculate variance, skewness, kurtosis, and entropy\n",
    "        variance = np.var(LL)\n",
    "        skewness = np.mean((LL - np.mean(LL))**3) / np.std(LL)**3\n",
    "        kurtosis = np.mean((LL - np.mean(LL))**4) / np.std(LL)**4\n",
    "        entropy = -np.sum(LL * np.log2(LL + 1e-10))\n",
    "\n",
    "        # Append features to arrays\n",
    "        variance_array.append(variance)\n",
    "        skewness_array.append(skewness)\n",
    "        kurtosis_array.append(kurtosis)\n",
    "        entropy_array.append(entropy)\n",
    "\n",
    "# Convert arrays to numpy arrays\n",
    "variance_array = np.array(variance_array)\n",
    "skewness_array = np.array(skewness_array)\n",
    "kurtosis_array = np.array(kurtosis_array)\n",
    "entropy_array = np.array(entropy_array)\n",
    "\n",
    "# Print the extracted features\n",
    "print(f\"Variance: {variance_array}\")\n",
    "print(f\"Skewness: {skewness_array}\")\n",
    "print(f\"Kurtosis: {kurtosis_array}\")\n",
    "print(f\"Entropy: {entropy_array}\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame to store the features\n",
    "df_2000_fake = pd.DataFrame({\n",
    "    \"Variance\": variance_array,\n",
    "    \"Skewness\": skewness_array,\n",
    "    \"Kurtosis\": kurtosis_array,\n",
    "    \"Entropy\": entropy_array\n",
    "})\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 500 Fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance: [ 6261.86009381 21776.72758585 13962.02807699 21795.73047168\n",
      " 21795.73047168 21873.7092187  21873.7092187  21873.7092187\n",
      " 21873.7092187  16212.28520892 16212.28520892 16212.28520892\n",
      " 16212.28520892 13962.02807699 21776.72758585 21776.72758585\n",
      " 21776.72758585 13962.02807699 13962.02807699 10442.01273829\n",
      " 10442.01273829 10442.01273829 10442.01273829  9503.31018858\n",
      "  9503.31018858 18752.89356918  9503.31018858  9503.31018858\n",
      "  8295.12121342  8295.12121342  8295.12121342  9277.33778826\n",
      "  9277.33778826  9277.33778826  9277.33778826  9808.36577466\n",
      "  7804.49783304  7800.87141994  7800.87141994  7800.87141994\n",
      "  7800.87141994 10626.05342926 10626.05342926 10626.05342926\n",
      " 10626.05342926  7804.49783304  7804.49783304  7804.49783304\n",
      "  7804.49783304  7804.49783304  7804.49783304  8315.30594076\n",
      "  8315.30594076  8315.30594076 17926.72626134  8315.30594076\n",
      "  7680.75561507 17926.72626134  7680.75561507 23356.61589318\n",
      " 23356.61589318 23356.61589318 23356.61589318 22843.66061198\n",
      " 22843.66061198 17926.72626134 22843.66061198 22843.66061198\n",
      " 17304.70142736 17304.70142736 17304.70142736 17304.70142736\n",
      " 13700.98848727 13700.98848727 13700.98848727 13700.98848727\n",
      " 17926.72626134 14341.55087727 14341.55087727 14341.55087727\n",
      " 14341.55087727 11385.4016153  11385.4016153  11385.4016153\n",
      " 11385.4016153  21795.73047168 21795.73047168  7620.26811184\n",
      " 10442.01273829  9503.31018858  8295.12121342  9808.36577466\n",
      "  7800.87141994 10626.05342926  6748.54560957  9277.33778826\n",
      " 10987.37297695 10729.0109033  15615.29077772  9429.14068164\n",
      "  7804.49783304  9277.33778826  8315.30594076  8315.30594076\n",
      "  8315.30594076  9277.33778826  8315.30594076  8554.17784763\n",
      " 10729.0109033   6901.00964906  8554.17784763 10987.37297695\n",
      " 10729.0109033   7804.49783304  6730.16124527 11516.22752812\n",
      " 11108.90893218 14899.48704653 10095.04056117  7493.36473238\n",
      " 13773.28378104  7804.49783304  7804.49783304  8315.30594076\n",
      "  8315.30594076  8315.30594076  8315.30594076 17304.70142736\n",
      " 17304.70142736 17304.70142736 17304.70142736 13700.98848727\n",
      " 13700.98848727 13700.98848727 13700.98848727 11385.4016153\n",
      "  9277.33778826  9277.33778826  9277.33778826  9277.33778826\n",
      " 21873.7092187  21873.7092187  16212.28520892 16212.28520892\n",
      " 16212.28520892 21776.72758585 17926.72626134 17926.72626134\n",
      " 13962.02807699 13962.02807699 13962.02807699 13962.02807699\n",
      " 10442.01273829  9503.31018858  9503.31018858  8295.12121342\n",
      " 10626.05342926 10626.05342926  7804.49783304  7804.49783304\n",
      "  9124.43492871]\n",
      "Skewness: [ 2.21713566  1.11806495  0.98742725  1.1183157   1.1183157   0.52844775\n",
      "  0.52844775  0.52844775  0.52844775  0.98857013  0.98857013  0.98857013\n",
      "  0.98857013  0.98742725  1.11806495  1.11806495  1.11806495  0.98742725\n",
      "  0.98742725 -0.72403154 -0.72403154 -0.72403154 -0.72403154 -1.26373185\n",
      " -1.26373185  1.13341731 -1.26373185 -1.26373185 -1.40328391 -1.40328391\n",
      " -1.40328391 -0.42436941 -0.42436941 -0.42436941 -0.42436941 -1.2257066\n",
      " -0.12582178 -1.55027958 -1.55027958 -1.55027958 -1.55027958 -0.72681973\n",
      " -0.72681973 -0.72681973 -0.72681973 -0.12582178 -0.12582178 -0.12582178\n",
      " -0.12582178 -0.12582178 -0.12582178 -0.14944356 -0.14944356 -0.14944356\n",
      "  0.51395168 -0.14944356 -0.35027333  0.51395168 -0.35027333  0.52728439\n",
      "  0.52728439  0.52728439  0.52728439  0.206181    0.206181    0.51395168\n",
      "  0.206181    0.206181    0.42758969  0.42758969  0.42758969  0.42758969\n",
      "  1.68287689  1.68287689  1.68287689  1.68287689  0.51395168  0.59834611\n",
      "  0.59834611  0.59834611  0.59834611  1.45779798  1.45779798  1.45779798\n",
      "  1.45779798  1.1183157   1.1183157  -0.50263696 -0.72403154 -1.26373185\n",
      " -1.40328391 -1.2257066  -1.55027958 -0.72681973 -0.92440787 -0.42436941\n",
      " -0.99426127 -0.31241453 -0.56235684 -1.27919172 -0.12582178 -0.42436941\n",
      " -0.14944356 -0.14944356 -0.14944356 -0.42436941 -0.14944356 -1.06908288\n",
      " -0.31241453 -0.80437667 -1.06908288 -0.99426127 -0.31241453 -0.12582178\n",
      " -0.03253166 -0.17409424 -0.19992564 -0.34158327 -0.29086544 -0.8070822\n",
      " -0.47157237 -0.12582178 -0.12582178 -0.14944356 -0.14944356 -0.14944356\n",
      " -0.14944356  0.42758969  0.42758969  0.42758969  0.42758969  1.68287689\n",
      "  1.68287689  1.68287689  1.68287689  1.45779798 -0.42436941 -0.42436941\n",
      " -0.42436941 -0.42436941  0.52844775  0.52844775  0.98857013  0.98857013\n",
      "  0.98857013  1.11806495  0.51395168  0.51395168  0.98742725  0.98742725\n",
      "  0.98742725  0.98742725 -0.72403154 -1.26373185 -1.26373185 -1.40328391\n",
      " -0.72681973 -0.72681973 -0.12582178 -0.12582178 -0.33573611]\n",
      "Kurtosis: [11.3456497   3.10648438  2.7548767   3.10905195  3.10905195  2.1085851\n",
      "  2.1085851   2.1085851   2.1085851   2.70451937  2.70451937  2.70451937\n",
      "  2.70451937  2.7548767   3.10648438  3.10648438  3.10648438  2.7548767\n",
      "  2.7548767   3.29437304  3.29437304  3.29437304  3.29437304  4.22420303\n",
      "  4.22420303  3.05636038  4.22420303  4.22420303  4.65058535  4.65058535\n",
      "  4.65058535  1.78728245  1.78728245  1.78728245  1.78728245  4.2205092\n",
      "  2.27897215  5.29672348  5.29672348  5.29672348  5.29672348  3.27142743\n",
      "  3.27142743  3.27142743  3.27142743  2.27897215  2.27897215  2.27897215\n",
      "  2.27897215  2.27897215  2.27897215  2.34126049  2.34126049  2.34126049\n",
      "  2.14976213  2.34126049  2.15868983  2.14976213  2.15868983  1.75254982\n",
      "  1.75254982  1.75254982  1.75254982  1.52191593  1.52191593  2.14976213\n",
      "  1.52191593  1.52191593  1.64417283  1.64417283  1.64417283  1.64417283\n",
      "  5.56200381  5.56200381  5.56200381  5.56200381  2.14976213  3.04146022\n",
      "  3.04146022  3.04146022  3.04146022  4.43227192  4.43227192  4.43227192\n",
      "  4.43227192  3.10905195  3.10905195  2.3696013   3.29437304  4.22420303\n",
      "  4.65058535  4.2205092   5.29672348  3.27142743  3.72437453  1.78728245\n",
      "  3.77384775  2.39516982  2.31438603  3.6643752   2.27897215  1.78728245\n",
      "  2.34126049  2.34126049  2.34126049  1.78728245  2.34126049  3.72590187\n",
      "  2.39516982  2.63023934  3.72590187  3.77384775  2.39516982  2.27897215\n",
      "  2.1895507   2.01588283  3.16824689  1.89750611  2.19765773  3.15129697\n",
      "  2.39980817  2.27897215  2.27897215  2.34126049  2.34126049  2.34126049\n",
      "  2.34126049  1.64417283  1.64417283  1.64417283  1.64417283  5.56200381\n",
      "  5.56200381  5.56200381  5.56200381  4.43227192  1.78728245  1.78728245\n",
      "  1.78728245  1.78728245  2.1085851   2.1085851   2.70451937  2.70451937\n",
      "  2.70451937  3.10648438  2.14976213  2.14976213  2.7548767   2.7548767\n",
      "  2.7548767   2.7548767   3.29437304  4.22420303  4.22420303  4.65058535\n",
      "  3.27142743  3.27142743  2.27897215  2.27897215  2.04053143]\n",
      "Entropy: [-1.16663508e+09 -3.07187783e+09 -2.87177626e+08 -3.06658542e+09\n",
      " -3.06658542e+09 -4.06624079e+09 -4.06624079e+09 -4.06624079e+09\n",
      " -4.06624079e+09 -2.60393273e+08 -2.60393273e+08 -2.60393273e+08\n",
      " -2.60393273e+08 -2.87177626e+08 -3.07187783e+09 -3.07187783e+09\n",
      " -3.07187783e+09 -2.87177626e+08 -2.87177626e+08 -9.06403291e+08\n",
      " -9.06403291e+08 -9.06403291e+08 -9.06403291e+08 -5.36640185e+08\n",
      " -5.36640185e+08 -3.56478825e+09 -5.36640185e+08 -5.36640185e+08\n",
      " -1.69107333e+09 -1.69107333e+09 -1.69107333e+09 -1.30478412e+08\n",
      " -1.30478412e+08 -1.30478412e+08 -1.30478412e+08 -5.29771914e+08\n",
      " -8.67306029e+07 -1.71181014e+09 -1.71181014e+09 -1.71181014e+09\n",
      " -1.71181014e+09 -9.02334447e+08 -9.02334447e+08 -9.02334447e+08\n",
      " -9.02334447e+08 -8.67306029e+07 -8.67306029e+07 -8.67306029e+07\n",
      " -8.67306029e+07 -8.67306029e+07 -8.67306029e+07 -9.01068609e+07\n",
      " -9.01068609e+07 -9.01068609e+07 -4.90906129e+09 -9.01068609e+07\n",
      " -6.52926330e+07 -4.90906129e+09 -6.52926330e+07 -3.96246664e+09\n",
      " -3.96246664e+09 -3.96246664e+09 -3.96246664e+09 -5.10747616e+09\n",
      " -5.10747616e+09 -4.90906129e+09 -5.10747616e+09 -5.10747616e+09\n",
      " -3.18000936e+08 -3.18000936e+08 -3.18000936e+08 -3.18000936e+08\n",
      " -2.73192535e+09 -2.73192535e+09 -2.73192535e+09 -2.73192535e+09\n",
      " -4.90906129e+09 -3.96291486e+09 -3.96291486e+09 -3.96291486e+09\n",
      " -3.96291486e+09 -2.27031639e+08 -2.27031639e+08 -2.27031639e+08\n",
      " -2.27031639e+08 -3.06658542e+09 -3.06658542e+09 -5.15192350e+08\n",
      " -9.06403291e+08 -5.36640185e+08 -1.69107333e+09 -5.29771914e+08\n",
      " -1.71181014e+09 -9.02334447e+08 -5.49378768e+08 -1.30478412e+08\n",
      " -1.04842426e+08 -2.15876788e+08 -3.18044875e+07 -5.00411497e+07\n",
      " -8.67306029e+07 -1.30478412e+08 -9.01068609e+07 -9.01068609e+07\n",
      " -9.01068609e+07 -1.30478412e+08 -9.01068609e+07 -1.05689954e+08\n",
      " -2.15876788e+08 -9.25404080e+07 -1.05689954e+08 -1.04842426e+08\n",
      " -2.15876788e+08 -8.67306029e+07 -3.16499984e+07 -2.72465929e+07\n",
      " -2.06136121e+08 -4.91022445e+07 -2.87529690e+07 -1.01266821e+08\n",
      " -1.54211059e+08 -8.67306029e+07 -8.67306029e+07 -9.01068609e+07\n",
      " -9.01068609e+07 -9.01068609e+07 -9.01068609e+07 -3.18000936e+08\n",
      " -3.18000936e+08 -3.18000936e+08 -3.18000936e+08 -2.73192535e+09\n",
      " -2.73192535e+09 -2.73192535e+09 -2.73192535e+09 -2.27031639e+08\n",
      " -1.30478412e+08 -1.30478412e+08 -1.30478412e+08 -1.30478412e+08\n",
      " -4.06624079e+09 -4.06624079e+09 -2.60393273e+08 -2.60393273e+08\n",
      " -2.60393273e+08 -3.07187783e+09 -4.90906129e+09 -4.90906129e+09\n",
      " -2.87177626e+08 -2.87177626e+08 -2.87177626e+08 -2.87177626e+08\n",
      " -9.06403291e+08 -5.36640185e+08 -5.36640185e+08 -1.69107333e+09\n",
      " -9.02334447e+08 -9.02334447e+08 -8.67306029e+07 -8.67306029e+07\n",
      " -8.78490883e+07]\n",
      "          Variance  Skewness  Kurtosis       Entropy\n",
      "0     10210.680969 -1.134840  3.884406 -5.243729e+08\n",
      "1      4599.779892 -0.226215  2.940177 -2.363746e+07\n",
      "2      3924.414624 -0.400361  2.884012 -2.628262e+07\n",
      "3     10210.680969 -1.134840  3.884406 -5.243729e+08\n",
      "4     10210.680969 -1.134840  3.884406 -5.243729e+08\n",
      "...            ...       ...       ...           ...\n",
      "1671  12077.664843 -0.592525  2.701257 -7.562985e+07\n",
      "1672  12164.772167 -0.743464  2.843340 -6.609715e+07\n",
      "1673  12077.664843 -0.592525  2.701257 -7.562985e+07\n",
      "1674  12077.664843 -0.592525  2.701257 -7.562985e+07\n",
      "1675  12077.664843 -0.592525  2.701257 -7.562985e+07\n",
      "\n",
      "[1676 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pywt\n",
    "\n",
    "# Define the path to the image dataset folder\n",
    "dataset_folder = r'D:\\RIDS DATASET\\500 FAKE' \n",
    "\n",
    "# Initialize empty arrays to store features\n",
    "variance_array = []\n",
    "skewness_array = []\n",
    "kurtosis_array = []\n",
    "entropy_array = []\n",
    "\n",
    "# Iterate over each image in the dataset folder\n",
    "for filename in os.listdir(dataset_folder):\n",
    "    if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "        image_path = os.path.join(dataset_folder, filename)\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Read image in grayscale\n",
    "\n",
    "        # Perform wavelet transform (using Haar wavelet)\n",
    "        coeffs = pywt.dwt2(image, 'db1')\n",
    "        LL, (LH, HL, HH) = coeffs\n",
    "\n",
    "        # Calculate variance, skewness, kurtosis, and entropy\n",
    "        variance = np.var(LL)\n",
    "        skewness = np.mean((LL - np.mean(LL))**3) / np.std(LL)**3\n",
    "        kurtosis = np.mean((LL - np.mean(LL))**4) / np.std(LL)**4\n",
    "        entropy = -np.sum(LL * np.log2(LL + 1e-10))\n",
    "\n",
    "        # Append features to arrays\n",
    "        variance_array.append(variance)\n",
    "        skewness_array.append(skewness)\n",
    "        kurtosis_array.append(kurtosis)\n",
    "        entropy_array.append(entropy)\n",
    "\n",
    "# Convert arrays to numpy arrays\n",
    "variance_array = np.array(variance_array)\n",
    "skewness_array = np.array(skewness_array)\n",
    "kurtosis_array = np.array(kurtosis_array)\n",
    "entropy_array = np.array(entropy_array)\n",
    "\n",
    "# Print the extracted features\n",
    "print(f\"Variance: {variance_array}\")\n",
    "print(f\"Skewness: {skewness_array}\")\n",
    "print(f\"Kurtosis: {kurtosis_array}\")\n",
    "print(f\"Entropy: {entropy_array}\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame to store the features\n",
    "df_500_fake = pd.DataFrame({\n",
    "    \"Variance\": variance_array,\n",
    "    \"Skewness\": skewness_array,\n",
    "    \"Kurtosis\": kurtosis_array,\n",
    "    \"Entropy\": entropy_array\n",
    "})\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LBP Feature Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from skimage.feature import local_binary_pattern\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform LBP feature extraction on an image\n",
    "def extract_lbp_features(image_path, points=24, radius=3):\n",
    "    # Read the image in grayscale\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    # Compute the LBP image\n",
    "    lbp_image = local_binary_pattern(image, points, radius, method='uniform')\n",
    "    # Calculate the histogram\n",
    "    (hist, _) = np.histogram(lbp_image.ravel(), bins=np.arange(0, points + 3), range=(0, points + 2))\n",
    "    # Normalize the histogram\n",
    "    hist = hist.astype(\"float\")\n",
    "    hist /= (hist.sum() + 1e-7)\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to iterate through all images in a folder and extract LBP features\n",
    "def process_folder(folder_path):\n",
    "    # List to hold all LBP features for each image\n",
    "    lbp_features_list = []\n",
    "    # Iterate through all files in the folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        # Construct the full path to the image\n",
    "        image_path = os.path.join(folder_path, file_name)\n",
    "        # Check if the file is an image\n",
    "        if os.path.isfile(image_path) and image_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            # Extract LBP features and append them to the list\n",
    "            lbp_features_list.append(extract_lbp_features(image_path))\n",
    "    # Convert the list of features to a numpy array\n",
    "    lbp_features_array = np.array(lbp_features_list)\n",
    "    return lbp_features_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'path_to_your_image_folder' with the actual path to your folder containing images\n",
    "folder_path_2000_1 = r'D:\\RIDS DATASET\\2000 Real'\n",
    "# Process the folder and extract LBP features\n",
    "features_array_2000_1 = process_folder(folder_path_2000_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_array_2000_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model  # TensorFlow is required for Keras to work\n",
    "from PIL import Image, ImageOps  # Install pillow instead of PIL\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing the directory\n",
    "import os\n",
    "import numpy as np\n",
    "os.getcwd()\n",
    "os.chdir(\"C:/Users/aksha/Downloads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable scientific notation for clarity\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = load_model(r\"C:\\Users\\aksha\\Downloads\\keras_Model.h5\", compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the labels\n",
    "class_names = open(\"labels.txt\", \"r\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the array of the right shape to feed into the keras model\n",
    "# The 'length' or number of images you can put into the array is\n",
    "# determined by the first position in the shape tuple, in this case 1\n",
    "data = np.ndarray(shape=(1, 224, 224, 3), dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace this with the path to your image\n",
    "image = Image.open(r\"C:\\Users\\aksha\\Downloads\\RIDS\\RIDS IMAGE DATASET\\2000 FAKE\\2000_f2.jpg\").convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resizing the image to be at least 224x224 and then cropping from the center\n",
    "size = (224, 224)\n",
    "image = ImageOps.fit(image, size, Image.Resampling.LANCZOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn the image into a numpy array\n",
    "image_array = np.asarray(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the image\n",
    "normalized_image_array = (image_array.astype(np.float32) / 127.5) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image into the array\n",
    "data[0] = normalized_image_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    }
   ],
   "source": [
    "# Predicts the model\n",
    "prediction = model.predict(data)\n",
    "index = np.argmax(prediction)\n",
    "class_name = class_names[index]\n",
    "confidence_score = prediction[0][index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: 2000 Fake\n",
      "Confidence Score: 0.94941604\n"
     ]
    }
   ],
   "source": [
    "# Print prediction and confidence score\n",
    "print(\"Class:\", class_name[2:], end=\"\")\n",
    "print(\"Confidence Score:\", confidence_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Front-End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "from PIL import Image, ImageTk\n",
    "import numpy as np\n",
    "import cv2  # For webcam capture\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox, ttk  # ttk for styled widgets\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model loading, image processing (from your code) \n",
    "def show_welcome_screen():\n",
    "    # Create elements within the main window\n",
    "    welcome_label = tk.Label(window, text=\"Welcome to the Fake Currency Detector\")\n",
    "    welcome_label.pack(pady=15) \n",
    "    continue_button = tk.Button(window, text=\"Click to Continue\", command=lambda: [welcome_label.destroy(), continue_button.destroy()])\n",
    "    continue_button.pack(pady=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# History Tracking (Simplified)\n",
    "history = []\n",
    "\n",
    "def save_result(image_path, prediction, confidence):\n",
    "    history.append({\n",
    "        'image_path': image_path,\n",
    "        'prediction': prediction,\n",
    "        'confidence': confidence\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_history():\n",
    "    history_window = tk.Toplevel(window)\n",
    "    history_window.title(\"Detection History\")\n",
    "\n",
    "    # Treeview for History Display\n",
    "    tree = ttk.Treeview(history_window, columns=('Image Path', 'Prediction', 'Confidence'))\n",
    "    tree.heading('#0', text='Index')\n",
    "    tree.heading('Image Path', text='Image Path')\n",
    "    tree.heading('Prediction', text='Prediction')\n",
    "    tree.heading('Confidence', text='Confidence')\n",
    "    tree.pack()\n",
    "\n",
    "    # Populate the Treeview\n",
    "    for i, record in enumerate(history):\n",
    "        tree.insert('', tk.END, text=str(i+1), values=(record['image_path'], record['prediction'], record['confidence']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_labels():\n",
    "    model = load_model(\"keras_Model.h5\", compile=False)\n",
    "    class_names = open(\"labels.txt\", \"r\").readlines()\n",
    "    return model, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image_path):\n",
    "    # 1. Load Image\n",
    "    image = Image.open(image_path).convert(\"RGB\")  # Ensure RGB format\n",
    "\n",
    "    # 2. Resize\n",
    "    size = (224, 224)  # Target size for your model \n",
    "    image = ImageOps.fit(image, size, Image.Resampling.LANCZOS)\n",
    "\n",
    "    # 3. Convert to Numpy Array\n",
    "    image_array = np.asarray(image)\n",
    "\n",
    "    # 4. Normalize\n",
    "    normalized_image_array = (image_array.astype(np.float32) / 127.5) - 1\n",
    "\n",
    "    # 5. Prepare Data\n",
    "    data = np.ndarray(shape=(1, 224, 224, 3), dtype=np.float32)\n",
    "    data[0] = normalized_image_array\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frontend and Logic\n",
    "def open_image():\n",
    "    image_path = filedialog.askopenfilename()\n",
    "    if image_path:\n",
    "        try:\n",
    "            image_data = process_image(image_path)\n",
    "            make_prediction(image_data)\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"Image Loading Error: {e}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_image():\n",
    "    cam = cv2.VideoCapture(0)\n",
    "    ret, frame = cam.read()\n",
    "    cam.release()\n",
    "\n",
    "    if ret:\n",
    "        try: \n",
    "            # Adjust frame processing as needed\n",
    "            image_data = process_image(frame)\n",
    "            make_prediction(image_data)\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"Webcam error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(image_data):\n",
    "    prediction = model.predict(image_data)\n",
    "    index = np.argmax(prediction)\n",
    "    class_name = class_names[index]\n",
    "    confidence_score = prediction[0][index]\n",
    "    result_label.config(text=f\"Class: {class_name[2:]}\\nConfidence Score: {confidence_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No file or directory found at keras_Model.h5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m history_btn \u001b[38;5;241m=\u001b[39m ttk\u001b[38;5;241m.\u001b[39mButton(window, text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mView History\u001b[39m\u001b[38;5;124m\"\u001b[39m, command\u001b[38;5;241m=\u001b[39mview_history)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Load model and labels on startup\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m model, class_names \u001b[38;5;241m=\u001b[39m load_model_and_labels()  \n\u001b[0;32m     16\u001b[0m window\u001b[38;5;241m.\u001b[39mmainloop()\n",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m, in \u001b[0;36mload_model_and_labels\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model_and_labels\u001b[39m():\n\u001b[1;32m----> 2\u001b[0m     model \u001b[38;5;241m=\u001b[39m load_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras_Model.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m     class_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, class_names\n",
      "File \u001b[1;32mc:\\anaconda3\\Lib\\site-packages\\keras\\saving\\saving_api.py:212\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    205\u001b[0m         filepath,\n\u001b[0;32m    206\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m    207\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[0;32m    208\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[0;32m    209\u001b[0m     )\n\u001b[0;32m    211\u001b[0m \u001b[38;5;66;03m# Legacy case.\u001b[39;00m\n\u001b[1;32m--> 212\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m legacy_sm_saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    213\u001b[0m     filepath, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects, \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    214\u001b[0m )\n",
      "File \u001b[1;32mc:\\anaconda3\\Lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\anaconda3\\Lib\\site-packages\\keras\\saving\\legacy\\save.py:230\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath_str, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mexists(filepath_str):\n\u001b[1;32m--> 230\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\n\u001b[0;32m    231\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo file or directory found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    232\u001b[0m         )\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39misdir(filepath_str):\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m saved_model_load\u001b[38;5;241m.\u001b[39mload(\n\u001b[0;32m    236\u001b[0m             filepath_str, \u001b[38;5;28mcompile\u001b[39m, options\n\u001b[0;32m    237\u001b[0m         )\n",
      "\u001b[1;31mOSError\u001b[0m: No file or directory found at keras_Model.h5"
     ]
    }
   ],
   "source": [
    "window = tk.Tk()\n",
    "window.title(\"Fake Currency Detector\")\n",
    "\n",
    "select_btn = tk.Button(window, text=\"Select Image\", command=open_image, bg=\"lightblue\")\n",
    "capture_btn = tk.Button(window, text=\"Live Capture\", command=capture_image, bg=\"lightgreen\")\n",
    "result_label = tk.Label(window, text=\"Result: \", font=('Arial', 14))\n",
    "\n",
    "select_btn.grid(row=0, column=0, padx=10, pady=10)\n",
    "capture_btn.grid(row=0, column=1, padx=10, pady=10)\n",
    "result_label.grid(row=1, column=0, columnspan=2, padx=10, pady=10)\n",
    "select_btn = ttk.Button(window, text=\"Select Image\", command=open_image)\n",
    "capture_btn = ttk.Button(window, text=\"Live Capture\", command=capture_image)\n",
    "history_btn = ttk.Button(window, text=\"View History\", command=view_history)\n",
    "# Load model and labels on startup\n",
    "model, class_names = load_model_and_labels()  \n",
    "window.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolation Upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.interpolate import interp1d \n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the columns to upsample\n",
    "columns_to_upsample = ['variance', 'skewness', 'curtosis', 'entropy', 'class']\n",
    "data = df[columns_to_upsample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the upsampling factor\n",
    "upsampling_factor = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the interpolation function for each column\n",
    "interpolated_data = {}\n",
    "for column in data.columns:\n",
    "    x = data.index.to_numpy()\n",
    "    y = data[column].to_numpy()\n",
    "    interpolation_function = interp1d(x, y, kind='linear')\n",
    "    new_index = np.linspace(x.min(), x.max(), num=len(x) * upsampling_factor)\n",
    "    upsampled_data = interpolation_function(new_index)\n",
    "    interpolated_data['upsampled_' + column] = upsampled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame with the upsampled data\n",
    "df_upsampled = pd.DataFrame(interpolated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the upsampled data to an Excel file\n",
    "df_upsampled.to_excel('upsampled_data.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, MaxAbsScaler, Normalizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('final smote.csv')\n",
    "data.columns\n",
    "df=data[['Variance', 'Skewness', 'Kurtosis', 'Entropy', 'Class']]\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target variable\n",
    "X = df.drop('Class', axis=1)  # Drop the target column\n",
    "y = df['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE\n",
    "oversampler = SMOTE()  \n",
    "X_resampled, y_resampled = oversampler.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check balanced classes (optional):\n",
    "print(y_resampled.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build your model using the new, balanced dataset\n",
    "# Concatenate X_resampled and y_resampled to create a new DataFrame\n",
    "resampled_df = pd.concat([pd.DataFrame(X_resampled, columns=X.columns), pd.Series(y_resampled, name='Class')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the new DataFrame\n",
    "print(resampled_df.head())\n",
    "\n",
    "resampled_df.to_csv('SMOTE resampled_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pywt\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_wavelet_features(image_path):\n",
    "  # Load the currency image\n",
    "  image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "  # Apply DWT using Daubechies 1 (DB1) wavelet\n",
    "  coeffs = pywt.dwt2(image, 'db1')\n",
    "  approx_coefficients = coeffs[0]\n",
    "\n",
    "  # Calculate variance, skewness, and kurtosis\n",
    "  variance = np.var(approx_coefficients)\n",
    "  skewness = np.mean((approx_coefficients - np.mean(approx_coefficients))**3) / np.std(approx_coefficients)**3\n",
    "  kurtosis = np.mean((approx_coefficients - np.mean(approx_coefficients))**4) / np.std(approx_coefficients)**4\n",
    "\n",
    "  # Calculate entropy\n",
    "  entropy = scipy.stats.entropy(approx_coefficients.flatten())\n",
    "\n",
    "  # Create an array with the extracted features\n",
    "  feature_array = np.array([variance, skewness, kurtosis, entropy])\n",
    "\n",
    "  return feature_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature matrix: (1257, 4)\n"
     ]
    }
   ],
   "source": [
    "folder_path_10 =r'D:\\Images Classifications\\Rs.10' \n",
    "\n",
    "\n",
    "def process_folder(folder_path_10):\n",
    "    \"\"\"Processes all images in a folder and extracts wavelet features.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing images.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array with shape (number_of_images, number_of_features) \n",
    "                    containing extracted features for all images.\n",
    "    \"\"\"\n",
    "\n",
    "    all_features = []  # Array to  store features of all images\n",
    "\n",
    "    for filename in os.listdir(folder_path_10):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):  # Check for image file\n",
    "            image_path = os.path.join(folder_path_10, filename)\n",
    "            features = extract_wavelet_features(image_path)\n",
    "            all_features.append(features)\n",
    "\n",
    "    return np.array(all_features)\n",
    "\n",
    "# Example usage\n",
    "\n",
    "features_matrix_10 = process_folder(folder_path_10)\n",
    "print(\"Shape of feature matrix:\", features_matrix_10.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature matrix: (1437, 4)\n"
     ]
    }
   ],
   "source": [
    "folder_path_20 =r'D:\\Images Classifications\\Rs.20' \n",
    "\n",
    "\n",
    "def process_folder(folder_path_20):\n",
    "    \"\"\"Processes all images in a folder and extracts wavelet features.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing images.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array with shape (number_of_images, number_of_features) \n",
    "                    containing extracted features for all images.\n",
    "    \"\"\"\n",
    "\n",
    "    all_features = []  # Array to  store features of all images\n",
    "\n",
    "    for filename in os.listdir(folder_path_20):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):  # Check for image file\n",
    "            image_path = os.path.join(folder_path_20, filename)\n",
    "            features = extract_wavelet_features(image_path)\n",
    "            all_features.append(features)\n",
    "\n",
    "    return np.array(all_features)\n",
    "\n",
    "# Example usage\n",
    "\n",
    "features_matrix_20 = process_folder(folder_path_20)\n",
    "print(\"Shape of feature matrix:\", features_matrix_20.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature matrix: (1429, 4)\n"
     ]
    }
   ],
   "source": [
    "folder_path_50 =r'D:\\Images Classifications\\Rs.50' \n",
    "\n",
    "\n",
    "def process_folder(folder_path_50):\n",
    "    \"\"\"Processes all images in a folder and extracts wavelet features.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing images.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array with shape (number_of_images, number_of_features) \n",
    "                    containing extracted features for all images.\n",
    "    \"\"\"\n",
    "\n",
    "    all_features = []  # Array to  store features of all images\n",
    "\n",
    "    for filename in os.listdir(folder_path_50):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):  # Check for image file\n",
    "            image_path = os.path.join(folder_path_50, filename)\n",
    "            features = extract_wavelet_features(image_path)\n",
    "            all_features.append(features)\n",
    "\n",
    "    return np.array(all_features)\n",
    "\n",
    "# Example usage\n",
    "\n",
    "features_matrix_50 = process_folder(folder_path_50)\n",
    "print(\"Shape of feature matrix:\", features_matrix_50.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature matrix: (1835, 4)\n"
     ]
    }
   ],
   "source": [
    "folder_path_100 =r'D:\\Images Classifications\\Rs.100' \n",
    "\n",
    "\n",
    "def process_folder(folder_path_100):\n",
    "    \"\"\"Processes all images in a folder and extracts wavelet features.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing images.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array with shape (number_of_images, number_of_features) \n",
    "                    containing extracted features for all images.\n",
    "    \"\"\"\n",
    "\n",
    "    all_features = []  # Array to  store features of all images\n",
    "\n",
    "    for filename in os.listdir(folder_path_100):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):  # Check for image file\n",
    "            image_path = os.path.join(folder_path_100, filename)\n",
    "            features = extract_wavelet_features(image_path)\n",
    "            all_features.append(features)\n",
    "\n",
    "    return np.array(all_features)\n",
    "\n",
    "# Example usage\n",
    "\n",
    "features_matrix_100 = process_folder(folder_path_100)\n",
    "print(\"Shape of feature matrix:\", features_matrix_100.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature matrix: (1599, 4)\n"
     ]
    }
   ],
   "source": [
    "folder_path_200 =r'D:\\Images Classifications\\Rs.200' \n",
    "\n",
    "\n",
    "def process_folder(folder_path_200):\n",
    "    \"\"\"Processes all images in a folder and extracts wavelet features.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing images.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array with shape (number_of_images, number_of_features) \n",
    "                    containing extracted features for all images.\n",
    "    \"\"\"\n",
    "\n",
    "    all_features = []  # Array to  store features of all images\n",
    "\n",
    "    for filename in os.listdir(folder_path_200):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):  # Check for image file\n",
    "            image_path = os.path.join(folder_path_200, filename)\n",
    "            features = extract_wavelet_features(image_path)\n",
    "            all_features.append(features)\n",
    "\n",
    "    return np.array(all_features)\n",
    "\n",
    "# Example usage\n",
    "\n",
    "features_matrix_200 = process_folder(folder_path_200)\n",
    "print(\"Shape of feature matrix:\", features_matrix_200.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature matrix: (1388, 4)\n"
     ]
    }
   ],
   "source": [
    "folder_path_500 =r'D:\\Images Classifications\\Rs.500' \n",
    "\n",
    "\n",
    "def process_folder(folder_path_500):\n",
    "    \"\"\"Processes all images in a folder and extracts wavelet features.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing images.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array with shape (number_of_images, number_of_features) \n",
    "                    containing extracted features for all images.\n",
    "    \"\"\"\n",
    "\n",
    "    all_features = []  # Array to  store features of all images\n",
    "\n",
    "    for filename in os.listdir(folder_path_500):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):  # Check for image file\n",
    "            image_path = os.path.join(folder_path_500, filename)\n",
    "            features = extract_wavelet_features(image_path)\n",
    "            all_features.append(features)\n",
    "\n",
    "    return np.array(all_features)\n",
    "\n",
    "# Example usage\n",
    "\n",
    "features_matrix_500 = process_folder(folder_path_500)\n",
    "print(\"Shape of feature matrix:\", features_matrix_500.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For 2000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path_2000 =r'D:\\Images Classifications\\Rs.2000' \n",
    "\n",
    "\n",
    "def process_folder(folder_path_2000):\n",
    "    \"\"\"Processes all images in a folder and extracts wavelet features.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing images.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array with shape (number_of_images, number_of_features) \n",
    "                    containing extracted features for all images.\n",
    "    \"\"\"\n",
    "\n",
    "    all_features = []  # Array to  store features of all images\n",
    "\n",
    "    for filename in os.listdir(folder_path_2000):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):  # Check for image file\n",
    "            image_path = os.path.join(folder_path_2000, filename)\n",
    "            features = extract_wavelet_features(image_path)\n",
    "            all_features.append(features)\n",
    "\n",
    "    return np.array(all_features)\n",
    "\n",
    "# Example usage\n",
    "\n",
    "features_matrix_2000 = process_folder(folder_path_2000)\n",
    "print(\"Shape of feature matrix:\", features_matrix_2000.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "def detect_outliers_zscore(data):\n",
    "    \"\"\"Detects outliers using the z-score method.\"\"\"\n",
    "    threshold = 3  # Common threshold for z-scores\n",
    "\n",
    "    outliers = []\n",
    "    means = data.mean()\n",
    "    std_devs = data.std()\n",
    "\n",
    "    for i, row in data.iterrows():\n",
    "        z_scores = [(row[col] - means[col]) / std_devs[col] for col in data.columns] \n",
    "        if any(abs(z) > threshold for z in z_scores):  \n",
    "            outliers.append(i)\n",
    "\n",
    "    return outliers\n",
    "\n",
    "# Load the CSV Data\n",
    "df = pd.read_excel(r'C:\\Users\\aksha\\Downloads\\RIDS\\Sahitya upsample.xlsx')\n",
    "\n",
    "# Detect Outliers using Z-scores\n",
    "outlier_indices = detect_outliers_zscore(df)\n",
    "\n",
    "# Print outlier indices (optional)\n",
    "print(\"Outlier Indices:\", outlier_indices)\n",
    "\n",
    "# Remove Outliers\n",
    "df_clean = df.drop(outlier_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset + ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding Accuracies of Fake/Real Dataset(Applied nothing)\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"C:/Users/DELL/Desktop/RIDS sem 4/data/rids_final.csv\", index_col=False)\n",
    "labelencoder = LabelEncoder()\n",
    "y = labelencoder.fit_transform(data[\"Class\"])\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=pd.DataFrame()\n",
    "X['Variance']=data['Variance']\n",
    "X['Skewness']=data['Skewness']\n",
    "X['Kurtosis']=data['Kurtosis']\n",
    "X['Entropy']=data['Entropy']\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.15,random_state=(42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "logistic_regression= LogisticRegression()\n",
    "logistic_regression.fit(X_train,y_train)\n",
    "y_pred=logistic_regression.predict(X_test)\n",
    "logistic_acc = metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Entrophy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier(criterion = \"entropy\")\n",
    "store = model.fit(X_train, y_train)\n",
    "predt = store.predict(X_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier(criterion = \"gini\")\n",
    "store = model.fit(X_train, y_train)\n",
    "predt = store.predict(X_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "linear_clf = svm.LinearSVC()\n",
    "linear_clf.fit(X_train, y_train)\n",
    "predt = linear_clf.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score as accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train,y_train)\n",
    "pred=knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "y_pred1 = rf_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracies\n",
    "print(\"Decision tree gini\",\":\",model_gini.score(X_test,predt)*100,\"%\")\n",
    "print(\"Decision tree entropy\",\":\", model.score(X_test,predt)*100,\"%\")\n",
    "print(\"Logistic\",\": \",logistic_acc*100,\"%\")\n",
    "print(\"SVM: \",accuracy(y_test,predt)*100,\"%\")  \n",
    "print(\"KNN: \",accuracy(y_test,pred)*100,\"%\")\n",
    "print(\"Random Forest: \", accuracy_score(y_test, y_pred1)*100)\n",
    "print(\"__________________________________________________________________________________________________\")\n",
    "print(\"___________________________________________________________________________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Accuracies of Fake/Real Dataset(Applied Normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "scaler = Normalizer()\n",
    "Variance_scaled1 = scaler.fit_transform(data[\"Variance\"].values.reshape(-1, 1))\n",
    "Skewness_scaled1 = scaler.fit_transform(data[\"Skewness\"].values.reshape(-1, 1))\n",
    "Kurtosis_scaled1 = scaler.fit_transform(data[\"Kurtosis\"].values.reshape(-1, 1))\n",
    "Entropy_scaled1 = scaler.fit_transform(data[\"Entropy\"].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the feature matrix X\n",
    "X = pd.DataFrame({\n",
    "    \"Variance_scaled\": Variance_scaled1.flatten(),\n",
    "    \"Skewness_scaled\": Skewness_scaled1.flatten(),\n",
    "    \"Kurtosis_scaled\": Kurtosis_scaled1.flatten(),\n",
    "    \"Entropy_scaled\": Entropy_scaled1.flatten()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25,random_state=(42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "logistic_regression= LogisticRegression()\n",
    "logistic_regression.fit(X_train,y_train)\n",
    "y_pred=logistic_regression.predict(X_test)\n",
    "logistic_acc = metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier(criterion = \"entropy\")\n",
    "store = model.fit(X_train, y_train)\n",
    "predt = store.predict(X_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decison tree gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gini = DecisionTreeClassifier(criterion = \"gini\")\n",
    "store_gini = model_gini.fit(X_train, y_train)\n",
    "predt_gini = model_gini.predict(X_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "linear_clf = svm.LinearSVC()\n",
    "linear_clf.fit(X_train, y_train)\n",
    "predt = linear_clf.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score as accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train,y_train)\n",
    "pred=knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "y_pred1 = rf_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracies\n",
    "print(\"Decision tree gini\",\":\",model_gini.score(X_test,predt)*100,\"%\")\n",
    "print(\"Decision tree entropy\",\":\", model.score(X_test,predt)*100,\"%\")\n",
    "print(\"Logistic\",\": \",logistic_acc*100,\"%\")\n",
    "print(\"SVM: \",accuracy(y_test,predt)*100,\"%\")  \n",
    "print(\"KNN: \",accuracy(y_test,pred)*100,\"%\")\n",
    "print(\"Random Forest: \", accuracy_score(y_test, y_pred1)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Front-End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fake_currency_model():\n",
    "    model = load_model(r'C:\\Users\\aksha\\Downloads\\fd\\converted_keras (2)\\keras_Model.h5', compile=False)\n",
    "    with open(r'C:\\Users\\aksha\\Downloads\\RIDS\\fd\\converted_keras (2)\\labels.txt') as f:\n",
    "        class_names = f.readlines()\n",
    "    return model, class_names\n",
    "\n",
    "def load_currency_classification_model():\n",
    "    model = load_model(r'C:\\Users\\aksha\\Downloads\\ic\\keras\\keras_Model.h5', compile=False)\n",
    "    with open(r'C:\\Users\\aksha\\Downloads\\RIDS\\ic\\keras\\labels.txt') as f:\n",
    "        class_names = f.readlines()\n",
    "    return model, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img, size=(224, 224)):\n",
    "    img = ImageOps.fit(img, size, Image.Resampling.LANCZOS)\n",
    "    img_array = np.asarray(img)\n",
    "    normalized_array = (img_array.astype(np.float32) / 127.5) - 1\n",
    "    return np.expand_dims(normalized_array, axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_fake_currency(image_input):\n",
    "    if image_input:\n",
    "        img_path = image_input  \n",
    "        img = Image.open(img_path).convert(\"RGB\")  \n",
    "        model, class_names = load_fake_currency_model()\n",
    "        img_data = preprocess_image(img)\n",
    "        prediction = model.predict(img_data)[0]\n",
    "        class_index = np.argmax(prediction)\n",
    "        class_name = class_names[class_index][2:].strip()\n",
    "        #confidence_score = prediction[class_index]\n",
    "        return f\"Class: {class_name}\"\n",
    "    else:\n",
    "        return \"Please provide an image for detection.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_currency(image_input):\n",
    "    if image_input:\n",
    "        img_path = image_input  \n",
    "        img = Image.open(img_path).convert(\"RGB\")  \n",
    "        model, class_names = load_currency_classification_model()\n",
    "        img_data = preprocess_image(img)\n",
    "        prediction = model.predict(img_data)[0]\n",
    "        class_index = np.argmax(prediction)\n",
    "        class_name = class_names[class_index][2:].strip()\n",
    "        #confidence_score = prediction[class_index]\n",
    "        return f\"Class: {class_name} \"\n",
    "    else:\n",
    "        return \"Please provide an image for classification\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Indian Currency Classification app\")\n",
    "\n",
    "    with gr.Tab(\"Fake Currency Detection\"):\n",
    "        with gr.Row():\n",
    "            image_input = gr.Image(type=\"filepath\")\n",
    "            camera = gr.Image(sources=\"webcam\")  # Not 'Video' anymore\n",
    "            #capture_button = gr.Button(\"Capture Image\")\n",
    "            #camera_button = gr.Button(\"Turn Camera Off\") \n",
    "        output = gr.Textbox(label=\"Prediction\")\n",
    "        detect_button = gr.Button(\"Detect\")\n",
    "\n",
    "    with gr.Tab(\"Currency Classification\"):\n",
    "        with gr.Row():\n",
    "            image_input2 = gr.Image(type=\"filepath\")\n",
    "            camera2 = gr.Image(sources=\"webcam\") \n",
    "            #capture_button2 = gr.Button(\"Capture Image\")\n",
    "            #camera_button2 = gr.Button(\"Turn Camera Off\") \n",
    "        output2 = gr.Textbox(label=\"Prediction\")\n",
    "        classify_button = gr.Button(\"Classify\")\n",
    "\n",
    "    # Event Handling\n",
    "    camera_on = True\n",
    "\n",
    "    def toggle_camera(state):\n",
    "        global camera_on\n",
    "        camera_on = not state\n",
    "        camera.sources = \"webcam\" if camera_on else None  \n",
    "        camera2.sources = \"webcam\" if camera_on else None\n",
    "        return camera_on\n",
    "\n",
    "    def capture_and_predict(img): \n",
    "        # Convert the captured frame to a PIL Image (replace if needed)\n",
    "        img = Image.fromarray(img)  \n",
    "\n",
    "        # Call your detection/classification functions\n",
    "        if detect_button.is_clicked:  \n",
    "            result = detect_fake_currency(img)\n",
    "            return result\n",
    "        elif classify_button.is_clicked:\n",
    "            result = classify_currency(img)\n",
    "            return result\n",
    "        else:\n",
    "            return \"Please select detection or classification.\"\n",
    "\n",
    "    detect_button.click(detect_fake_currency, inputs=image_input, outputs=output)\n",
    "    classify_button.click(classify_currency, inputs=image_input2, outputs=output2)\n",
    "    #camera_button.click(toggle_camera, inputs=[camera_button], outputs=camera_button)\n",
    "    #camera_button2.click(toggle_camera, inputs=[camera_button2], outputs=camera_button2) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rerunning server... use `close()` to stop if you need to change `launch()` parameters.\n",
      "----\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
